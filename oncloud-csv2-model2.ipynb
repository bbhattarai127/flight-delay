{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Predicting Airplane Delays\n",
    "\n",
    "The goals of this notebook are:\n",
    "- Process and create a dataset from downloaded ZIP files\n",
    "- Exploratory data analysis (EDA)\n",
    "- Establish a baseline model and improve it\n",
    "\n",
    "## Introduction to business scenario\n",
    "You work for a travel booking website that is working to improve the customer experience for flights that were delayed. The company wants to create a feature to let customers know if the flight will be delayed due to weather when the customers are booking the flight to or from the busiest airports for domestic travel in the US. \n",
    "\n",
    "You are tasked with solving part of this problem by leveraging machine learning to identify whether the flight will be delayed due to weather. You have been given access to the a dataset of on-time performance of domestic flights operated by large air carriers. You can use this data to train a machine learning model to predict if the flight is going to be delayed for the busiest airports.\n",
    "\n",
    "### Dataset\n",
    "The provided dataset contains scheduled and actual departure and arrival times reported by certified US air carriers that account for at least 1 percent of domestic scheduled passenger revenues. The data was collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS). The dataset contains date, time, origin, destination, airline, distance, and delay status of flights for flights between 2014 and 2018.\n",
    "The data are in 60 compressed files, where each file contains a CSV for the flight details in a month for the five years (from 2014 - 2018). The data can be downloaded from this [link](https://ucstaff-my.sharepoint.com/:f:/g/personal/ibrahim_radwan_canberra_edu_au/Er0nVreXmihEmtMz5qC5kVIB81-ugSusExPYdcyQTglfLg?e=bNO312). Please download the data files and place them on a relative path. Dataset(s) used in this assignment were compiled by the Office of Airline Information, Bureau of Transportation Statistics (BTS), Airline On-Time Performance Data, available with the following [link](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the environment \n",
    "\n",
    "Use one of the labs which we have practised on with the Amazon Sagemakers where you perform the following steps:\n",
    "1. Start a lab.\n",
    "2. Create a notebook instance and name it \"oncloudproject\".\n",
    "3. Increase the used memory to 25 GB from the additional configurations.\n",
    "4. Open Jupyter Lab and upload this notebook into it.\n",
    "5. Upload the two combined CVS files (combined_csv_v1.csv and combined_csv_v2.csv), which you created in Part A of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Build and evaluate ensembe models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use xgboost estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "6. write down your observation on the difference between the performance of using the simple and ensemble models.\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model Training and Evaluation Pipeline  \n",
    "Set up, train, and evaluate a machine learning model using SageMaker, with essential libraries for data processing and metrics evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.transformer import Transformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the SageMaker Environment and Preparing Data  \n",
    "Initialize the SageMaker session and prepare your dataset for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7991/2953908413.py:6: DtypeWarning: Columns (41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,100,101,102,103,104,105,106,107,108,109,110,111,112) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"combined_csv_v2.csv\")  # Load the dataset\n"
     ]
    }
   ],
   "source": [
    "# Set up SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"Your-SageMaker-Execution-Role-ARN\"  # Replace with your SageMaker role ARN\n",
    "\n",
    "# Step 1: Load and Split the Data\n",
    "data = pd.read_csv(\"combined_csv_v2.csv\")  # Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Dataset Columns  \n",
    "View the column names in the dataset for data preparation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['target', 'CRSDepTime', 'Cancelled', 'Diverted', 'Distance',\n",
      "       'DistanceGroup', 'ArrDelay', 'ArrDelayMinutes', 'target.1', 'AirTime',\n",
      "       ...\n",
      "       'OriginState_IL', 'OriginState_NC', 'OriginState_TX', 'DestState_CA',\n",
      "       'DestState_CO', 'DestState_GA', 'DestState_IL', 'DestState_NC',\n",
      "       'DestState_TX', 'isHoliday_True'],\n",
      "      dtype='object', length=113)\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Data Splitting  \n",
    "Define feature and target columns, split data into training, validation, and test sets, and save the sets for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7991/1895646515.py:10: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  train_data = pd.concat([y_train, X_train], axis=1)\n",
      "/tmp/ipykernel_7991/1895646515.py:12: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  test_data = pd.concat([y_test, X_test], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Define features and target using the correct column name\n",
    "X = data.drop(columns=['target'])  # Assuming 'target' is the correct target column\n",
    "y = data['target']\n",
    "\n",
    "# Split into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save training and testing datasets\n",
    "train_data = pd.concat([y_train, X_train], axis=1)\n",
    "train_data.to_csv(\"train.csv\", index=False, header=False)\n",
    "test_data = pd.concat([y_test, X_test], axis=1)\n",
    "test_data.to_csv(\"test.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Training Data to S3  \n",
    "Store your prepared training dataset in Amazon S3 for easy access during model training in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use SageMaker's default bucket\n",
    "s3_bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "# Upload training data to the default S3 bucket\n",
    "s3_input_train = sagemaker_session.upload_data(\"train.csv\", bucket=s3_bucket_name, key_prefix=\"xgboost/train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Train an XGBoost Model on SageMaker  \n",
    "Configure an XGBoost estimator with hyperparameters and initiate model training using your prepared data in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-09-34-02-262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-01 09:34:07 Starting - Starting the training job...\n",
      "2024-11-01 09:34:21 Starting - Preparing the instances for training...\n",
      "2024-11-01 09:34:44 Downloading - Downloading input data...\n",
      "2024-11-01 09:35:19 Downloading - Downloading the training image...\n",
      "2024-11-01 09:35:55 Training - Training image download completed. Training in progress..\u001b[34m[2024-11-01 09:35:59.832 ip-10-0-229-74.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[2024-11-01 09:36:00.233 ip-10-0-229-74.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-11-01 09:36:00.233 ip-10-0-229-74.ec2.internal:7 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-11-01 09:36:00.233 ip-10-0-229-74.ec2.internal:7 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-11-01 09:36:00.234 ip-10-0-229-74.ec2.internal:7 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-11-01 09:36:00.234 ip-10-0-229-74.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 68698 rows and 112 columns\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[2024-11-01 09:36:00.420 ip-10-0-229-74.ec2.internal:7 INFO hook.py:413] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2024-11-01 09:36:00.422 ip-10-0-229-74.ec2.internal:7 INFO hook.py:476] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:1.00000\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:1.00000\u001b[0m\n",
      "\n",
      "2024-11-01 09:36:23 Uploading - Uploading generated training model\n",
      "2024-11-01 09:36:23 Completed - Training job completed\n",
      "Training seconds: 100\n",
      "Billable seconds: 100\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Automatically retrieve the SageMaker execution role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Use the default S3 bucket for SageMaker\n",
    "s3_bucket_name = sagemaker_session.default_bucket()\n",
    "output_path = f\"s3://{s3_bucket_name}/xgboost/output\"\n",
    "\n",
    "# Step 2: Define and Train XGBoost Model\n",
    "xgb_estimator = Estimator(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", sagemaker_session.boto_region_name, \"1.2-1\"),\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=100,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"auc\"\n",
    ")\n",
    "\n",
    "# Train the model using the training data in S3\n",
    "xgb_estimator.fit({\"train\": TrainingInput(s3_input_train, content_type=\"csv\")})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Trained XGBoost Model  \n",
    "Deploy the trained XGBoost model to a SageMaker endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2024-11-01-09-36-50-451\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2024-11-01-09-36-50-451\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2024-11-01-09-36-50-451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# Step 3: Deploy the Model\n",
    "predictor = xgb_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Batch Transformation\n",
    "This section involves loading, cleaning, and saving the test dataset for use in the model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to test_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "# Step 1: Load and Clean the Data\n",
    "# Load the original test data file\n",
    "file_path = 'test.csv'  # Replace with the actual file path if needed\n",
    "test_data = pd.read_csv(file_path)\n",
    "\n",
    "# Replace boolean strings 'True'/'False' with 1 and 0 across the entire DataFrame\n",
    "test_data.replace({True: 1, False: 0, 'True': 1, 'False': 0}, inplace=True)\n",
    "\n",
    "# Verify that all values are numeric\n",
    "test_data = test_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop any rows containing NaN values introduced during conversion\n",
    "test_data_cleaned = test_data.dropna()\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_file_path = 'test_cleaned.csv'\n",
    "test_data_cleaned.to_csv(cleaned_file_path, index=False, header=False)\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Count Validation and Adjustment\n",
    "This section ensures that the test data has the correct number of features expected by the model, adjusting as necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted test data saved to test_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure we have the correct number of features by checking against the training feature count\n",
    "expected_feature_count = 112  # The feature count expected by the model\n",
    "\n",
    "# Adjust columns if necessary\n",
    "if test_data.shape[1] > expected_feature_count:\n",
    "    test_data = test_data.iloc[:, :expected_feature_count]  # Keep only the first 112 columns\n",
    "elif test_data.shape[1] < expected_feature_count:\n",
    "    print(f\"Warning: Test data has fewer columns ({test_data.shape[1]}) than expected ({expected_feature_count}).\")\n",
    "\n",
    "# Convert 'True'/'False' strings to 1/0 if they still exist\n",
    "test_data.replace({'True': 1, 'False': 0}, inplace=True)\n",
    "\n",
    "# Save the adjusted test data\n",
    "cleaned_file_path = 'test_cleaned.csv'\n",
    "test_data.to_csv(cleaned_file_path, index=False, header=False)\n",
    "print(f\"Adjusted test data saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transformation of Cleaned Test Data\n",
    "Proceeding with batch transformation using the adjusted test data uploaded to S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2024-11-01-10-40-44-856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................\u001b[34m[2024-11-01:10:45:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [21] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [21] [INFO] Listening at: unix:/tmp/gunicorn.sock (21)\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [21] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:42:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:10:45:42 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:10:45:42 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:43:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\n",
      "\u001b[34m[2024-11-01:10:45:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:44:INFO] Loading model file '/opt/ml/model/xgboost-model'\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:10:45:44 +0000] \"POST /invocations HTTP/1.1\" 200 328455 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2024-11-01:10:45:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2024-11-01:10:45:44:INFO] Loading model file '/opt/ml/model/xgboost-model'\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Nov/2024:10:45:44 +0000] \"POST /invocations HTTP/1.1\" 200 328455 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2024-11-01T10:45:42.846:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m[2024-11-01:10:45:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2024-11-01:10:45:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2024-11-01:10:45:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [21] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [21] [INFO] Listening at: unix:/tmp/gunicorn.sock (21)\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [21] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[34m[2024-11-01 10:45:38 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2024-11-01 10:45:38 +0000] [21] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2024-11-01 10:45:38 +0000] [21] [INFO] Listening at: unix:/tmp/gunicorn.sock (21)\u001b[0m\n",
      "\u001b[35m[2024-11-01 10:45:38 +0000] [21] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2024-11-01 10:45:38 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[35m[2024-11-01 10:45:38 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[35m[2024-11-01 10:45:38 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[35m[2024-11-01 10:45:38 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:42:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:10:45:42 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:10:45:42 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:43:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2024-11-01:10:45:42:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Nov/2024:10:45:42 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Nov/2024:10:45:42 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2024-11-01:10:45:43:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-11-01:10:45:44:INFO] Loading model file '/opt/ml/model/xgboost-model'\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:10:45:44 +0000] \"POST /invocations HTTP/1.1\" 200 328455 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2024-11-01:10:45:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2024-11-01:10:45:44:INFO] Loading model file '/opt/ml/model/xgboost-model'\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Nov/2024:10:45:44 +0000] \"POST /invocations HTTP/1.1\" 200 328455 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2024-11-01T10:45:42.846:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Batch transform output saved to: s3://sagemaker-us-east-1-590183778271/xgboost/output\n"
     ]
    }
   ],
   "source": [
    "# Now proceed with batch transform using this adjusted data\n",
    "import sagemaker\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Define the default bucket and upload cleaned test data to S3\n",
    "s3_bucket_name = sagemaker_session.default_bucket()\n",
    "s3_input_test = sagemaker_session.upload_data(cleaned_file_path, bucket=s3_bucket_name, key_prefix=\"xgboost/test\")\n",
    "\n",
    "# Assuming `predictor.endpoint_name` is the model endpoint created during training\n",
    "transformer = Transformer(\n",
    "    model_name=predictor.endpoint_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=f\"s3://{s3_bucket_name}/xgboost/output\"  # Specify output location in S3\n",
    ")\n",
    "\n",
    "# Perform batch transform\n",
    "transformer.transform(data=s3_input_test, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()\n",
    "\n",
    "print(f\"Batch transform output saved to: s3://{s3_bucket_name}/xgboost/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Predictions File or Directory\n",
    "This section ensures that any existing `predictions.csv` file or directory is removed before proceeding with the download of the new predictions file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of 'predictions.csv' directory: ['claim.smd', 'test_cleaned.csv.out']\n",
      "'predictions.csv' directory and its contents have been removed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Check if 'predictions.csv' exists and is a directory\n",
    "if os.path.exists(\"predictions.csv\"):\n",
    "    if os.path.isdir(\"predictions.csv\"):\n",
    "        # List contents of the directory for inspection\n",
    "        print(\"Contents of 'predictions.csv' directory:\", os.listdir(\"predictions.csv\"))\n",
    "        \n",
    "        # Remove the directory and all its contents\n",
    "        shutil.rmtree(\"predictions.csv\")\n",
    "        print(\"'predictions.csv' directory and its contents have been removed.\")\n",
    "    else:\n",
    "        # If it's a file, remove it directly\n",
    "        os.remove(\"predictions.csv\")\n",
    "        print(\"'predictions.csv' file removed.\")\n",
    "\n",
    "# Proceed with the previous code to download the predictions file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Evaluate Model Predictions\n",
    "In this section, we will download the predictions from S3, clean up any existing files, and calculate various performance metrics based on the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in output path: ['xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/claim.smd', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/collections/000000000/worker_0_collections.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/events/000000000000/000000000000_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/events/000000000010/000000000010_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/events/000000000020/000000000020_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/events/000000000030/000000000030_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/events/000000000040/000000000040_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/events/000000000050/000000000050_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/events/000000000060/000000000060_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/events/000000000070/000000000070_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/events/000000000080/000000000080_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/events/000000000090/000000000090_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/index/000000000/000000000000_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/index/000000000/000000000010_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/index/000000000/000000000020_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/index/000000000/000000000030_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/index/000000000/000000000040_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/index/000000000/000000000050_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/index/000000000/000000000060_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/index/000000000/000000000070_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/index/000000000/000000000080_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/index/000000000/000000000090_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/debug-output/training_job_end.ts', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/output/model.tar.gz', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/profiler-output/framework/training_job_end.ts', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/profiler-output/system/incremental/2024110109/1730453640.algo-1.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/profiler-output/system/incremental/2024110109/1730453700.algo-1.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/profiler-output/system/incremental/2024110109/1730453760.algo-1.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-09-34-02-262/profiler-output/system/training_job_end.ts', 'xgboost/output/test_cleaned.csv.out']\n",
      "Inconsistent lengths detected: y_pred (14716) vs y_test (14721)\n",
      "Predictions distribution (first 10 values): [1.18454394e-04 2.44390085e-05 2.44390085e-05 2.44390085e-05\n",
      " 2.44390085e-05 2.44390085e-05 2.44390085e-05 2.44390085e-05\n",
      " 2.44390085e-05 2.44390085e-05]\n",
      "Binary Predictions distribution: [14721]\n",
      "Performance metrics for Dataset V2:\n",
      "Accuracy: 0.76\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n",
      "AUC: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Initialize Boto3 S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Define S3 bucket and output prefix\n",
    "s3_bucket_name = sagemaker_session.default_bucket()\n",
    "output_prefix = \"xgboost/output\"\n",
    "\n",
    "# List files in the output directory to find the batch transform output file (.out)\n",
    "response = s3_client.list_objects_v2(Bucket=s3_bucket_name, Prefix=output_prefix)\n",
    "output_files = [content['Key'] for content in response.get('Contents', [])]\n",
    "print(\"Files in output path:\", output_files)\n",
    "\n",
    "# Locate the file ending in '.out' (the predictions file)\n",
    "predictions_file_key = next((file for file in output_files if file.endswith('.out')), None)\n",
    "\n",
    "# Clean up any pre-existing 'predictions.csv' directory or file to avoid conflicts\n",
    "if os.path.exists(\"predictions.csv\"):\n",
    "    if os.path.isdir(\"predictions.csv\"):\n",
    "        shutil.rmtree(\"predictions.csv\")  # Recursively remove the directory\n",
    "    else:\n",
    "        os.remove(\"predictions.csv\")  # Remove the file if it exists\n",
    "\n",
    "if predictions_file_key:\n",
    "    # Use a unique temporary name for download\n",
    "    temp_file = \"temp_predictions.csv\"\n",
    "    s3_client.download_file(s3_bucket_name, predictions_file_key, temp_file)\n",
    "    \n",
    "    # Rename the temporary file to 'predictions.csv'\n",
    "    os.rename(temp_file, \"predictions.csv\")\n",
    "\n",
    "    # Verify that 'predictions.csv' was created successfully\n",
    "    if os.path.isfile(\"predictions.csv\"):\n",
    "        try:\n",
    "            # Load predictions, skip empty lines\n",
    "            y_pred = np.loadtxt(\"predictions.csv\", delimiter=\",\")\n",
    "            y_pred = y_pred.flatten()\n",
    "\n",
    "            # Ensure lengths match between y_pred and y_test\n",
    "            if len(y_pred) != len(y_test):\n",
    "                print(f\"Inconsistent lengths detected: y_pred ({len(y_pred)}) vs y_test ({len(y_test)})\")\n",
    "                if len(y_pred) > len(y_test):\n",
    "                    y_pred = y_pred[:len(y_test)]\n",
    "                else:\n",
    "                    y_pred = np.pad(y_pred, (0, len(y_test) - len(y_pred)), 'constant', constant_values=0)\n",
    "\n",
    "            # Print out the prediction values to understand the distribution\n",
    "            print(\"Predictions distribution (first 10 values):\", y_pred[:10])\n",
    "            \n",
    "            # Dynamically find a threshold if needed, but start with a default of 0.5\n",
    "            threshold = 0.5\n",
    "            y_pred_binary = np.where(y_pred >= threshold, 1, 0)\n",
    "\n",
    "            # Check distribution of binary predictions\n",
    "            print(\"Binary Predictions distribution:\", np.bincount(y_pred_binary))\n",
    "\n",
    "            # Define a function to calculate and print evaluation metrics\n",
    "            def print_metrics(y_true, y_pred_binary):\n",
    "                print(f\"Accuracy: {accuracy_score(y_true, y_pred_binary):.2f}\")\n",
    "                print(f\"Precision: {precision_score(y_true, y_pred_binary):.2f}\")\n",
    "                print(f\"Recall: {recall_score(y_true, y_pred_binary):.2f}\")\n",
    "                print(f\"F1 Score: {f1_score(y_true, y_pred_binary):.2f}\")\n",
    "                print(f\"AUC: {roc_auc_score(y_true, y_pred):.2f}\")  # Use continuous y_pred for AUC\n",
    "\n",
    "            # Display performance metrics for the model on the test dataset\n",
    "            print(\"Performance metrics for Dataset V2:\")\n",
    "            print_metrics(y_test, y_pred_binary)\n",
    "        \n",
    "        except ValueError as e:\n",
    "            print(f\"Error loading predictions from 'predictions.csv': {e}\")\n",
    "    else:\n",
    "        print(\"Download failed or predictions.csv does not exist.\")\n",
    "else:\n",
    "    print(\"Prediction file (.out) not found. Please check the batch transform job for errors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metrics for Dataset V2 indicate a concerning model evaluation. While the accuracy stands at 0.76, which might suggest that a significant portion of predictions are correct, the precision, recall, and F1 score all show values of 0.00. This is alarming, as it suggests that while some predictions may be correct, the model fails to effectively identify positive cases. Precision indicates the number of true positive predictions made out of all positive predictions, and a score of 0.00 signifies that no true positive cases were found. Similarly, the recall metric, which measures the model's ability to identify actual positive cases, also being 0.00 indicates a complete failure in detecting positive instances.\n",
    "\n",
    "The F1 score, which combines precision and recall into a single metric, is also 0.00, underscoring the model's ineffectiveness. The AUC (Area Under the Curve) score of 0.50 implies that the model's performance is equivalent to random guessing, meaning it lacks discriminative power. This set of metrics calls for an immediate review of the model's training process, feature selection, and potentially its architecture, as the current state does not adequately serve the classification task at hand. Steps should be taken to enhance the model's ability to generalize and identify relevant patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up the SageMaker Endpoint\n",
    "In this step, we will delete the SageMaker endpoint to release resources and avoid incurring unnecessary charges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: sagemaker-xgboost-2024-11-01-09-36-50-451\n",
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-xgboost-2024-11-01-09-36-50-451\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Clean up the endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Comments\n",
    "In comparing the two models—Linear Learner and XGBoost—used to predict flight delays, we observe distinctive benefits and limitations in each approach. These differences underscore trade-offs in computational resources, accuracy, interpretability, and deployment suitability, each of which influences the practical choice of model.\n",
    "\n",
    "### Resource Efficiency and Computational Complexity\n",
    "The Linear Learner model offers a lightweight, computationally efficient option, ideal for scenarios where resources or processing power are limited. This model is fast to train and deploy, making it an appropriate choice when time constraints or low computational resources are a concern. However, its simplicity limits its ability to capture the non-linear, intricate relationships typical of flight delay data. For instance, flight delays often involve complex factors like weather patterns, seasonal trends, and airport congestion. While Linear Learner can serve as a reliable baseline, it may struggle to identify these nuanced relationships, potentially leading to reduced predictive accuracy and underfitting.\n",
    "\n",
    "On the other hand, XGBoost, an ensemble model, requires more computational resources and time due to its gradient-boosting approach, which iteratively builds decision trees and corrects errors from previous trees. This process enables XGBoost to capture complex data relationships and interactions, making it effective for high-dimensional datasets like flight delay data. Despite the computational cost, XGBoost’s robustness in handling complex datasets often translates into higher predictive accuracy. Nevertheless, this model’s resource demands may limit its use in scenarios with tight computational constraints or where rapid deployment is critical.\n",
    "\n",
    "### Accuracy and Model Performance\n",
    "In terms of accuracy, XGBoost generally outperforms Linear Learner due to its ability to model non-linear relationships and interactions among features. Flight delay prediction relies on understanding various interdependent factors, and XGBoost’s boosting technique allows it to iteratively improve its predictions by correcting errors, resulting in a model that can capture these dynamics effectively. However, the complexity of XGBoost introduces the risk of overfitting, where the model performs well on training data but fails to generalize to unseen data. Addressing this requires careful tuning and regularization techniques to maintain a balance between accuracy and generalizability.\n",
    "\n",
    "Linear Learner, while faster and simpler, might underfit, especially if the relationships in the data are complex. Its limitations become apparent when multiple factors interact, such as weather conditions varying by season or airport traffic affecting delay likelihood. Nevertheless, Linear Learner’s speed and ease of implementation make it valuable as a quick, interpretable model for initial evaluations or in scenarios where predictive accuracy is less critical.\n",
    "\n",
    "### Interpretability and Practical Application\n",
    "Interpretability is often a priority in predictive modeling, especially in high-stakes fields like air travel, where insights into predictions can influence significant decisions. Linear Learner excels here, as it provides clear, interpretable coefficients that indicate the relationship between each feature and flight delay probability. This transparency allows stakeholders to understand which factors, such as weather or airport congestion, are contributing to delays.\n",
    "\n",
    "Conversely, XGBoost, while powerful, lacks inherent interpretability due to its complex, tree-based structure. While tools like SHAP values can aid in interpreting XGBoost’s outputs, these additional interpretability steps introduce complexity, which may be a drawback in applications requiring straightforward, accessible explanations.\n",
    "\n",
    "### Conclusion\n",
    "In summary, Linear Learner’s simplicity, speed, and interpretability make it a viable choice for baseline modeling or scenarios with limited resources. It offers practical insights into delay factors, though at the expense of predictive accuracy on complex datasets. XGBoost, with its superior accuracy and robustness, is well-suited for applications prioritizing accuracy and capable of supporting computationally intensive models. The choice between these models depends on the specific application needs, balancing interpretability, resource constraints, and accuracy requirements. Both models have valuable roles, with the ideal model determined by project priorities and the demands of the deployment environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

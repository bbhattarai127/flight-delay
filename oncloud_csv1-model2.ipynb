{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Predicting Airplane Delays\n",
    "\n",
    "The goals of this notebook are:\n",
    "- Process and create a dataset from downloaded ZIP files\n",
    "- Exploratory data analysis (EDA)\n",
    "- Establish a baseline model and improve it\n",
    "\n",
    "## Introduction to business scenario\n",
    "You work for a travel booking website that is working to improve the customer experience for flights that were delayed. The company wants to create a feature to let customers know if the flight will be delayed due to weather when the customers are booking the flight to or from the busiest airports for domestic travel in the US. \n",
    "\n",
    "You are tasked with solving part of this problem by leveraging machine learning to identify whether the flight will be delayed due to weather. You have been given access to the a dataset of on-time performance of domestic flights operated by large air carriers. You can use this data to train a machine learning model to predict if the flight is going to be delayed for the busiest airports.\n",
    "\n",
    "### Dataset\n",
    "The provided dataset contains scheduled and actual departure and arrival times reported by certified US air carriers that account for at least 1 percent of domestic scheduled passenger revenues. The data was collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS). The dataset contains date, time, origin, destination, airline, distance, and delay status of flights for flights between 2014 and 2018.\n",
    "The data are in 60 compressed files, where each file contains a CSV for the flight details in a month for the five years (from 2014 - 2018). The data can be downloaded from this [link](https://ucstaff-my.sharepoint.com/:f:/g/personal/ibrahim_radwan_canberra_edu_au/Er0nVreXmihEmtMz5qC5kVIB81-ugSusExPYdcyQTglfLg?e=bNO312). Please download the data files and place them on a relative path. Dataset(s) used in this assignment were compiled by the Office of Airline Information, Bureau of Transportation Statistics (BTS), Airline On-Time Performance Data, available with the following [link](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the environment \n",
    "\n",
    "Use one of the labs which we have practised on with the Amazon Sagemakers where you perform the following steps:\n",
    "1. Start a lab.\n",
    "2. Create a notebook instance and name it \"oncloudproject\".\n",
    "3. Increase the used memory to 25 GB from the additional configurations.\n",
    "4. Open Jupyter Lab and upload this notebook into it.\n",
    "5. Upload the two combined CVS files (combined_csv_v1.csv and combined_csv_v2.csv), which you created in Part A of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1920112552.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    1. Split data into training, validation and testing sets (70% - 15% - 15%).\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Build and evaluate ensembe models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use xgboost estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "6. write down your observation on the difference between the performance of using the simple and ensemble models.\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Split Data\n",
    "Load the dataset (`combined_csv_v2.csv`) and split it into training, validation, and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"combined_csv_v1.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check and Clean NaN Values\n",
    "Check for NaN values in each column, remove rows with any NaN values, and verify that no NaNs remain. Finally, save the cleaned data back to the CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in each column:\n",
      " target    22540\n",
      "dtype: int64\n",
      "\n",
      "NaN values after cleaning:\n",
      " 0\n",
      "Saved cleaned data to 'combined_csv_v2_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "nan_counts = df.isna().sum()\n",
    "print(\"NaN values in each column:\\n\", nan_counts[nan_counts > 0])\n",
    "\n",
    "# Remove rows with any NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Verify that there are no NaN values left\n",
    "print(\"\\nNaN values after cleaning:\\n\", df.isna().sum().sum())  # Should output 0 if no NaNs are left\n",
    "\n",
    "# Save the cleaned data back to the CSV\n",
    "df.to_csv(\"combined_csv_v2_cleaned.csv\", index=False)\n",
    "print(\"Saved cleaned data to 'combined_csv_v2_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert and Split Data for Model Training\n",
    "Convert any `'TRUE'/'FALSE'` strings to Boolean values if necessary. Split the dataset into features and target, then further split into training (70%), validation (15%), and test (15%) sets. Save these splits to CSV files for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert any 'TRUE'/'FALSE' strings to Boolean values if necessary\n",
    "df.replace({'TRUE': True, 'FALSE': False}, inplace=True)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns=['target'])  # Replace 'target' with the actual target column name if different\n",
    "y = df['target']\n",
    "\n",
    "# Split data into training (70%) and temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split temp data into validation (15%) and test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save these to CSV for uploading to S3\n",
    "X_train.to_csv(\"X_train.csv\", index=False)\n",
    "y_train.to_csv(\"y_train.csv\", index=False)\n",
    "X_val.to_csv(\"X_val.csv\", index=False)\n",
    "y_val.to_csv(\"y_val.csv\", index=False)\n",
    "X_test.to_csv(\"X_test.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Check for NaN Values in S3 Files\n",
    "Download each data file from the `flight-delay1` S3 bucket, load it into a DataFrame, and check for any NaN values. Print the columns with NaN values if found, or confirm if there are none.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN values in X_train.csv\n",
      "No NaN values in y_train.csv\n",
      "No NaN values in X_val.csv\n",
      "No NaN values in y_val.csv\n",
      "No NaN values in X_test.csv\n",
      "No NaN values in y_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file names to check for NaN values\n",
    "files = [\"X_train.csv\", \"y_train.csv\", \"X_val.csv\", \"y_val.csv\", \"X_test.csv\", \"y_test.csv\"]\n",
    "\n",
    "# Function to check NaN values in each file\n",
    "for file in files:\n",
    "    # Load the file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Check for NaN values\n",
    "    nan_counts = df.isna().sum()\n",
    "    nan_columns = nan_counts[nan_counts > 0]\n",
    "    \n",
    "    if not nan_columns.empty:\n",
    "        print(f\"\\nNaN values found in {file}:\")\n",
    "        print(nan_columns)\n",
    "    else:\n",
    "        print(f\"No NaN values in {file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Check Column Names in S3 Files\n",
    "Download each data file from the `flight-delay1` S3 bucket, load it into a DataFrame, and print the column names for each file to verify consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column names in X_train.csv:\n",
      "['Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO', 'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5', 'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9', 'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12', 'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15', 'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18', 'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21', 'DepHourofDay_22', 'DepHourofDay_23']\n",
      "\n",
      "Column names in y_train.csv:\n",
      "['target']\n",
      "\n",
      "Column names in X_val.csv:\n",
      "['Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO', 'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5', 'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9', 'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12', 'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15', 'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18', 'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21', 'DepHourofDay_22', 'DepHourofDay_23']\n",
      "\n",
      "Column names in y_val.csv:\n",
      "['target']\n",
      "\n",
      "Column names in X_test.csv:\n",
      "['Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO', 'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5', 'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9', 'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12', 'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15', 'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18', 'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21', 'DepHourofDay_22', 'DepHourofDay_23']\n",
      "\n",
      "Column names in y_test.csv:\n",
      "['target']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file names to check for column names\n",
    "files = [\"X_train.csv\", \"y_train.csv\", \"X_val.csv\", \"y_val.csv\", \"X_test.csv\", \"y_test.csv\"]\n",
    "\n",
    "# Function to check column names in each file\n",
    "for file in files:\n",
    "    # Load the file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Print column names\n",
    "    print(f\"\\nColumn names in {file}:\")\n",
    "    print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column names in X_train.csv:\n",
      "['Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO', 'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5', 'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9', 'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12', 'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15', 'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18', 'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21', 'DepHourofDay_22', 'DepHourofDay_23']\n",
      "X_train.csv has 93 features.\n",
      "\n",
      "Column names in y_train.csv:\n",
      "['target']\n",
      "y_train.csv contains the target column with 1144913 rows.\n",
      "\n",
      "Column names in X_val.csv:\n",
      "['Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO', 'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5', 'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9', 'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12', 'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15', 'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18', 'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21', 'DepHourofDay_22', 'DepHourofDay_23']\n",
      "X_val.csv has 93 features.\n",
      "\n",
      "Column names in y_val.csv:\n",
      "['target']\n",
      "y_val.csv contains the target column with 245338 rows.\n",
      "\n",
      "Column names in X_test.csv:\n",
      "['Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO', 'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5', 'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9', 'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12', 'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15', 'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18', 'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21', 'DepHourofDay_22', 'DepHourofDay_23']\n",
      "X_test.csv has 93 features.\n",
      "\n",
      "Column names in y_test.csv:\n",
      "['target']\n",
      "y_test.csv contains the target column with 245339 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file names to check for column names\n",
    "files = {\n",
    "    \"X_train\": \"X_train.csv\",\n",
    "    \"y_train\": \"y_train.csv\",\n",
    "    \"X_val\": \"X_val.csv\",\n",
    "    \"y_val\": \"y_val.csv\",\n",
    "    \"X_test\": \"X_test.csv\",\n",
    "    \"y_test\": \"y_test.csv\"\n",
    "}\n",
    "\n",
    "# Function to summarize features and targets in each file\n",
    "for key, file in files.items():\n",
    "    # Load the file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Print column names\n",
    "    print(f\"\\nColumn names in {file}:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    # Summary: Count number of features or confirm target\n",
    "    if 'target' in df.columns:\n",
    "        print(f\"{file} contains the target column with {len(df)} rows.\")\n",
    "    else:\n",
    "        print(f\"{file} has {len(df.columns)} features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate columns found in X_val.csv: {'Distance', 'DepHourofDay_7', 'DayofMonth_31', 'DayofMonth_12', 'DayofMonth_27', 'Origin_DFW', 'DepHourofDay_22', 'Month_12', 'Month_10', 'DayOfWeek_3', 'DayofMonth_21', 'Origin_LAX', 'Month_7', 'DayofMonth_15', 'DayOfWeek_5', 'DayofMonth_2', 'DayofMonth_3', 'Month_2', 'Reporting_Airline_DL', 'DayofMonth_30', 'DepHourofDay_13', 'DepHourofDay_6', 'Dest_SFO', 'Dest_DFW', 'DayofMonth_14', 'DepHourofDay_1', 'Month_9', 'Quarter_3', 'DayofMonth_10', 'Month_4', 'Month_6', 'DayofMonth_6', 'DayofMonth_19', 'DayofMonth_24', 'DepHourofDay_8', 'DayofMonth_20', 'Dest_LAX', 'DayOfWeek_2', 'Quarter_4', 'Dest_IAH', 'Origin_ORD', 'Dest_CLT', 'DepHourofDay_18', 'DepHourofDay_5', 'DepHourofDay_20', 'DayofMonth_16', 'Origin_CLT', 'DayofMonth_28', 'DayofMonth_26', 'Reporting_Airline_OO', 'Month_5', 'DayOfWeek_4', 'DepHourofDay_17', 'Month_8', 'DayofMonth_11', 'Origin_DEN', 'Dest_ORD', 'DayofMonth_5', 'Reporting_Airline_WN', 'DayofMonth_8', 'DayofMonth_7', 'DepHourofDay_11', 'DepHourofDay_16', 'DayofMonth_13', 'Origin_SFO', 'Origin_IAH', 'Quarter_2', 'DepHourofDay_19', 'DepHourofDay_15', 'DayofMonth_18', 'DepHourofDay_12', 'DayofMonth_4', 'DayofMonth_22', 'Reporting_Airline_UA', 'DayofMonth_9', 'Dest_DEN', 'DayofMonth_23', 'Dest_PHX', 'Origin_PHX', 'DepHourofDay_4', 'DepHourofDay_23', 'DepHourofDay_10', 'DepHourofDay_9', 'DepHourofDay_21', 'DepHourofDay_2', 'DayOfWeek_7', 'DayofMonth_25', 'DayOfWeek_6', 'DayofMonth_29', 'Month_11', 'DayofMonth_17', 'Month_3', 'DepHourofDay_14'}\n",
      "Duplicate columns found in X_test.csv: {'Distance', 'DepHourofDay_7', 'DayofMonth_31', 'DayofMonth_12', 'DayofMonth_27', 'Origin_DFW', 'DepHourofDay_22', 'Month_12', 'Month_10', 'DayOfWeek_3', 'DayofMonth_21', 'Origin_LAX', 'Month_7', 'DayofMonth_15', 'DayOfWeek_5', 'DayofMonth_2', 'DayofMonth_3', 'Month_2', 'Reporting_Airline_DL', 'DayofMonth_30', 'DepHourofDay_13', 'DepHourofDay_6', 'Dest_SFO', 'Dest_DFW', 'DayofMonth_14', 'DepHourofDay_1', 'Month_9', 'Quarter_3', 'DayofMonth_10', 'Month_4', 'Month_6', 'DayofMonth_6', 'DayofMonth_19', 'DayofMonth_24', 'DepHourofDay_8', 'DayofMonth_20', 'Dest_LAX', 'DayOfWeek_2', 'Quarter_4', 'Dest_IAH', 'Origin_ORD', 'Dest_CLT', 'DepHourofDay_18', 'DepHourofDay_5', 'DepHourofDay_20', 'DayofMonth_16', 'Origin_CLT', 'DayofMonth_28', 'DayofMonth_26', 'Reporting_Airline_OO', 'Month_5', 'DayOfWeek_4', 'DepHourofDay_17', 'Month_8', 'DayofMonth_11', 'Origin_DEN', 'Dest_ORD', 'DayofMonth_5', 'Reporting_Airline_WN', 'DayofMonth_8', 'DayofMonth_7', 'DepHourofDay_11', 'DepHourofDay_16', 'DayofMonth_13', 'Origin_SFO', 'Origin_IAH', 'Quarter_2', 'DepHourofDay_19', 'DepHourofDay_15', 'DayofMonth_18', 'DepHourofDay_12', 'DayofMonth_4', 'DayofMonth_22', 'Reporting_Airline_UA', 'DayofMonth_9', 'Dest_DEN', 'DayofMonth_23', 'Dest_PHX', 'Origin_PHX', 'DepHourofDay_4', 'DepHourofDay_23', 'DepHourofDay_10', 'DepHourofDay_9', 'DepHourofDay_21', 'DepHourofDay_2', 'DayOfWeek_7', 'DayofMonth_25', 'DayOfWeek_6', 'DayofMonth_29', 'Month_11', 'DayofMonth_17', 'Month_3', 'DepHourofDay_14'}\n"
     ]
    }
   ],
   "source": [
    "# Function to check for duplicate columns across X files\n",
    "x_files = [\"X_train.csv\", \"X_val.csv\", \"X_test.csv\"]\n",
    "all_columns = set()\n",
    "\n",
    "for file in x_files:\n",
    "    df = pd.read_csv(file)\n",
    "    file_columns = set(df.columns)\n",
    "    duplicates = all_columns.intersection(file_columns)\n",
    "    if duplicates:\n",
    "        print(f\"Duplicate columns found in {file}: {duplicates}\")\n",
    "    all_columns.update(file_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types in X_train.csv:\n",
      "Distance           float64\n",
      "Quarter_2             bool\n",
      "Quarter_3             bool\n",
      "Quarter_4             bool\n",
      "Month_2               bool\n",
      "                    ...   \n",
      "DepHourofDay_19       bool\n",
      "DepHourofDay_20       bool\n",
      "DepHourofDay_21       bool\n",
      "DepHourofDay_22       bool\n",
      "DepHourofDay_23       bool\n",
      "Length: 93, dtype: object\n",
      "\n",
      "Data types in X_val.csv:\n",
      "Distance           float64\n",
      "Quarter_2             bool\n",
      "Quarter_3             bool\n",
      "Quarter_4             bool\n",
      "Month_2               bool\n",
      "                    ...   \n",
      "DepHourofDay_19       bool\n",
      "DepHourofDay_20       bool\n",
      "DepHourofDay_21       bool\n",
      "DepHourofDay_22       bool\n",
      "DepHourofDay_23       bool\n",
      "Length: 93, dtype: object\n",
      "\n",
      "Data types in X_test.csv:\n",
      "Distance           float64\n",
      "Quarter_2             bool\n",
      "Quarter_3             bool\n",
      "Quarter_4             bool\n",
      "Month_2               bool\n",
      "                    ...   \n",
      "DepHourofDay_19       bool\n",
      "DepHourofDay_20       bool\n",
      "DepHourofDay_21       bool\n",
      "DepHourofDay_22       bool\n",
      "DepHourofDay_23       bool\n",
      "Length: 93, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to print data types of columns in each X file\n",
    "for key, file in files.items():\n",
    "    if 'X' in key:  # Only for X files\n",
    "        df = pd.read_csv(file)\n",
    "        print(f\"\\nData types in {file}:\")\n",
    "        print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - nvidia\n",
      " - pytorch\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 24.7.1\n",
      "    latest version: 24.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/pytorch_p310\n",
      "\n",
      "  added / updated specs:\n",
      "    - xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0           8 KB  conda-forge\n",
      "    libxgboost-2.1.2           |   cpu_h3a1dfae_0         3.1 MB  conda-forge\n",
      "    numpy-2.1.2                |  py310hd6e36ab_0         7.5 MB  conda-forge\n",
      "    py-xgboost-2.1.2           | cpu_pyh15c3653_0         131 KB  conda-forge\n",
      "    xgboost-2.1.2              | cpu_pyhac85b48_0          15 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  conda-forge/linux-64::_py-xgboost-mutex-2.0-cpu_0 \n",
      "  joblib             conda-forge/noarch::joblib-1.4.2-pyhd8ed1ab_0 \n",
      "  libxgboost         conda-forge/linux-64::libxgboost-2.1.2-cpu_h3a1dfae_0 \n",
      "  numpy              conda-forge/linux-64::numpy-2.1.2-py310hd6e36ab_0 \n",
      "  py-xgboost         conda-forge/noarch::py-xgboost-2.1.2-cpu_pyh15c3653_0 \n",
      "  scikit-learn       conda-forge/linux-64::scikit-learn-1.5.2-py310h27f47ee_1 \n",
      "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.5.0-pyhc1e730c_0 \n",
      "  xgboost            conda-forge/noarch::xgboost-2.1.2-cpu_pyhac85b48_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "numpy-2.1.2          | 7.5 MB    |                                       |   0% \n",
      "libxgboost-2.1.2     | 3.1 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "py-xgboost-2.1.2     | 131 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "xgboost-2.1.2        | 15 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "numpy-2.1.2          | 7.5 MB    | 9                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "xgboost-2.1.2        | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "libxgboost-2.1.2     | 3.1 MB    | 1                                     |   1% \u001b[A\n",
      "\n",
      "py-xgboost-2.1.2     | 131 KB    | ####5                                 |  12% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "numpy-2.1.2          | 7.5 MB    | ###########1                          |  30% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 8 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "libxgboost-2.1.2     | 3.1 MB    | ###############1                      |  41% \u001b[A\n",
      "\n",
      "\n",
      "xgboost-2.1.2        | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "py-xgboost-2.1.2     | 131 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "py-xgboost-2.1.2     | 131 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "libxgboost-2.1.2     | 3.1 MB    | ##################################### | 100% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "XGBoost is successfully installed and imported!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install XGBoost\n",
    "!conda install -c conda-forge xgboost -y\n",
    "import xgboost as xgb\n",
    "print(\"XGBoost is successfully installed and imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [14:06:56] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232729175/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7959101321442256\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.99      0.89    194137\n",
      "         1.0       0.66      0.05      0.08     51201\n",
      "\n",
      "    accuracy                           0.80    245338\n",
      "   macro avg       0.73      0.52      0.48    245338\n",
      "weighted avg       0.77      0.80      0.72    245338\n",
      "\n",
      "Test Accuracy: 0.7940808432413925\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.99      0.88    193645\n",
      "         1.0       0.67      0.04      0.08     51694\n",
      "\n",
      "    accuracy                           0.79    245339\n",
      "   macro avg       0.73      0.52      0.48    245339\n",
      "weighted avg       0.77      0.79      0.72    245339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the data\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "X_val = pd.read_csv(\"X_val.csv\")\n",
    "y_val = pd.read_csv(\"y_val.csv\")\n",
    "X_test = pd.read_csv(\"X_test.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")\n",
    "\n",
    "# Prepare the data\n",
    "# Assuming the target variable is binary, adjust as necessary\n",
    "X_train = X_train.astype(float)  # Convert features to float if necessary\n",
    "X_val = X_val.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "# Train the model\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train.values.ravel())  # Flatten y_train\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost_model.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Import necessary libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Step 3: Load and train the model\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "\n",
    "model = xgb.XGBClassifier(eval_metric='logloss')\n",
    "model.fit(X_train, y_train.values.ravel())  # Flatten y_train\n",
    "joblib.dump(model, 'xgboost_model.joblib')  # Save the model locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Upload the model to the default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "model_artifact = f's3://{bucket_name}/xgboost_model.joblib'\n",
    "boto3.client('s3').upload_file('xgboost_model.joblib', bucket_name, 'xgboost_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Create a SageMaker model and deploy it\n",
    "role = get_execution_role()  # Get the execution role\n",
    "xgb_model = sagemaker.model.Model(\n",
    "    model_data=model_artifact,\n",
    "    role=role,\n",
    "    entry_point='inference.py',  # Ensure this script exists\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 6: Deploy the model to an endpoint\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of instances for the endpoint\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml.m5.large\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify the instance type\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxgboost-endpoint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/model.py:1698\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, model_reference_arn, **kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# existing single model endpoint path\u001b[39;00m\n\u001b[0;32m-> 1698\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_sagemaker_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_eula\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_eula\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_reference_arn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_reference_arn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1706\u001b[0m     serverless_inference_config_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1707\u001b[0m         serverless_inference_config\u001b[38;5;241m.\u001b[39m_to_request_dict() \u001b[38;5;28;01mif\u001b[39;00m is_serverless \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1708\u001b[0m     )\n\u001b[1;32m   1709\u001b[0m     production_variant \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mproduction_variant(\n\u001b[1;32m   1710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1711\u001b[0m         instance_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m         routing_config\u001b[38;5;241m=\u001b[39mrouting_config,\n\u001b[1;32m   1719\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/model.py:930\u001b[0m, in \u001b[0;36mModel._create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags, serverless_inference_config, accept_eula, model_reference_arn)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m model_package\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     container_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_container_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_eula\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_eula\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_reference_arn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_reference_arn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session, PipelineSession):\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;66;03m# _base_name, model_name are not needed under PipelineSession.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;66;03m# the model_data may be Pipeline variable\u001b[39;00m\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;66;03m# which may break the _base_name generation\u001b[39;00m\n\u001b[1;32m    942\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_base_name_if_needed(\n\u001b[1;32m    943\u001b[0m             image_uri\u001b[38;5;241m=\u001b[39mcontainer_def[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    944\u001b[0m             script_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir,\n\u001b[1;32m    945\u001b[0m             model_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_uri(),\n\u001b[1;32m    946\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/model.py:713\u001b[0m, in \u001b[0;36mModel.prepare_container_def\u001b[0;34m(self, instance_type, accelerator_type, serverless_inference_config, accept_eula, model_reference_arn)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_container_def\u001b[39m(\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    682\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m     model_reference_arn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    687\u001b[0m ):  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a dict created by ``sagemaker.container_def()``.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m    It is used for deploying this model to a specified instance type.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m        dict: A container definition object usable with the CreateModel API.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m     deploy_key_prefix \u001b[38;5;241m=\u001b[39m \u001b[43mfw_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_code_key_prefix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_uri\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m     deploy_env \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_config:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/fw_utils.py:597\u001b[0m, in \u001b[0;36mmodel_code_key_prefix\u001b[0;34m(code_location_key_prefix, model_name, image)\u001b[0m\n\u001b[1;32m    595\u001b[0m name_from_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/model_code/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_pipeline_variable(image):\n\u001b[0;32m--> 597\u001b[0m     name_from_image \u001b[38;5;241m=\u001b[39m \u001b[43msagemaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_from_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s3_path_join(code_location_key_prefix, model_name \u001b[38;5;129;01mor\u001b[39;00m name_from_image)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/utils.py:96\u001b[0m, in \u001b[0;36mname_from_image\u001b[0;34m(image, max_length)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname_from_image\u001b[39m(image, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m63\u001b[39m):\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a training job name based on the image name and a timestamp.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m        max_length (int): Maximum length for the resulting string (default: 63).\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m name_from_base(\u001b[43mbase_name_from_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/utils.py:166\u001b[0m, in \u001b[0;36mbase_name_from_image\u001b[0;34m(image, default_base_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     image_str \u001b[38;5;241m=\u001b[39m image\n\u001b[0;32m--> 166\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m^(.+/)?([^:/]+)(:[^:]+)?$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m base_name \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;28;01melse\u001b[39;00m image_str\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m base_name\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/re.py:190\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Step 6: Deploy the model to an endpoint\n",
    "predictor = xgb_model.deploy(\n",
    "    initial_instance_count=1,  # Number of instances for the endpoint\n",
    "    instance_type='ml.m5.large',  # Specify the instance type\n",
    "    endpoint_name='xgboost-endpoint',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Make predictions (sample data)\n",
    "sample_data = X_train.iloc[:5].values.tolist()  # Adjust this as necessary for your input\n",
    "predictions = predictor.predict(sample_data)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Predicting Airplane Delays\n",
    "\n",
    "The goals of this notebook are:\n",
    "- Process and create a dataset from downloaded ZIP files\n",
    "- Exploratory data analysis (EDA)\n",
    "- Establish a baseline model and improve it\n",
    "\n",
    "## Introduction to business scenario\n",
    "You work for a travel booking website that is working to improve the customer experience for flights that were delayed. The company wants to create a feature to let customers know if the flight will be delayed due to weather when the customers are booking the flight to or from the busiest airports for domestic travel in the US. \n",
    "\n",
    "You are tasked with solving part of this problem by leveraging machine learning to identify whether the flight will be delayed due to weather. You have been given access to the a dataset of on-time performance of domestic flights operated by large air carriers. You can use this data to train a machine learning model to predict if the flight is going to be delayed for the busiest airports.\n",
    "\n",
    "### Dataset\n",
    "The provided dataset contains scheduled and actual departure and arrival times reported by certified US air carriers that account for at least 1 percent of domestic scheduled passenger revenues. The data was collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS). The dataset contains date, time, origin, destination, airline, distance, and delay status of flights for flights between 2014 and 2018.\n",
    "The data are in 60 compressed files, where each file contains a CSV for the flight details in a month for the five years (from 2014 - 2018). The data can be downloaded from this [link](https://ucstaff-my.sharepoint.com/:f:/g/personal/ibrahim_radwan_canberra_edu_au/Er0nVreXmihEmtMz5qC5kVIB81-ugSusExPYdcyQTglfLg?e=bNO312). Please download the data files and place them on a relative path. Dataset(s) used in this assignment were compiled by the Office of Airline Information, Bureau of Transportation Statistics (BTS), Airline On-Time Performance Data, available with the following [link](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the environment \n",
    "\n",
    "Use one of the labs which we have practised on with the Amazon Sagemakers where you perform the following steps:\n",
    "1. Start a lab.\n",
    "2. Create a notebook instance and name it \"oncloudproject\".\n",
    "3. Increase the used memory to 25 GB from the additional configurations.\n",
    "4. Open Jupyter Lab and upload this notebook into it.\n",
    "5. Upload the two combined CVS files (combined_csv_v1.csv and combined_csv_v2.csv), which you created in Part A of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Build and evaluate ensembe models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use xgboost estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "6. write down your observation on the difference between the performance of using the simple and ensemble models.\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries for Data Preparation and Model Training\n",
    "# This section includes libraries for data manipulation, machine learning, and SageMaker integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.transformer import Transformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing SageMaker Session and Loading Data  \n",
    "# Setting Up Environment and Preparing the Dataset for Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11509/2576652941.py:6: DtypeWarning: Columns (64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"combined_csv_v1.csv\")  # Load the dataset\n"
     ]
    }
   ],
   "source": [
    "# Set up SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"Your-SageMaker-Execution-Role-ARN\"  # Replace with your SageMaker role ARN\n",
    "\n",
    "# Step 1: Load and Split the Data\n",
    "data = pd.read_csv(\"combined_csv_v1.csv\")  # Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Column Names\n",
    "## This section outputs the names of all columns in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['target', 'Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2',\n",
      "       'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8',\n",
      "       'Month_9', 'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2',\n",
      "       'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6',\n",
      "       'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10',\n",
      "       'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14',\n",
      "       'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18',\n",
      "       'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22',\n",
      "       'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26',\n",
      "       'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30',\n",
      "       'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4',\n",
      "       'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL',\n",
      "       'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN',\n",
      "       'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX',\n",
      "       'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN',\n",
      "       'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO',\n",
      "       'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5',\n",
      "       'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9',\n",
      "       'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12',\n",
      "       'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15',\n",
      "       'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18',\n",
      "       'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21',\n",
      "       'DepHourofDay_22', 'DepHourofDay_23'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset for Training and Testing\n",
    "This section defines the features and target variable, splits the dataset into training, validation, and test sets, and saves them to CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define features and target using the correct column name\n",
    "X = data.drop(columns=['target'])  # Assuming 'target' is the correct target column\n",
    "y = data['target']\n",
    "\n",
    "# Split into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save training and testing datasets\n",
    "train_data = pd.concat([y_train, X_train], axis=1)\n",
    "train_data.to_csv(\"train.csv\", index=False, header=False)\n",
    "test_data = pd.concat([y_test, X_test], axis=1)\n",
    "test_data.to_csv(\"test.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Training Data to Amazon S3\n",
    "This section utilizes SageMaker's default bucket to upload the prepared training data for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use SageMaker's default bucket\n",
    "s3_bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "# Upload training data to the default S3 bucket\n",
    "s3_input_train = sagemaker_session.upload_data(\"train.csv\", bucket=s3_bucket_name, key_prefix=\"xgboost/train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Training the XGBoost Model\n",
    "In this section, we configure and initiate the training process for the XGBoost model using SageMaker's Estimator. This involves specifying the model parameters and feeding the training data from S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-11-29-21-563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-01 11:29:22 Starting - Starting the training job...\n",
      "2024-11-01 11:29:36 Starting - Preparing the instances for training...\n",
      "2024-11-01 11:30:07 Downloading - Downloading input data...\n",
      "2024-11-01 11:30:28 Downloading - Downloading the training image...\n",
      "2024-11-01 11:31:19 Training - Training image download completed. Training in progress..\u001b[34m[2024-11-01 11:31:23.808 ip-10-2-126-124.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[2024-11-01 11:31:24.193 ip-10-2-126-124.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-11-01 11:31:24.194 ip-10-2-126-124.ec2.internal:7 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-11-01 11:31:24.194 ip-10-2-126-124.ec2.internal:7 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-11-01 11:31:24.195 ip-10-2-126-124.ec2.internal:7 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-11-01 11:31:24.195 ip-10-2-126-124.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 79124 rows and 93 columns\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.61788\u001b[0m\n",
      "\u001b[34m[2024-11-01 11:31:24.449 ip-10-2-126-124.ec2.internal:7 INFO hook.py:413] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2024-11-01 11:31:24.451 ip-10-2-126-124.ec2.internal:7 INFO hook.py:476] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.62747\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.64196\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.65411\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.65755\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.66075\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.66754\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.67107\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.67688\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.67993\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.68383\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.68792\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.69330\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.69420\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.69831\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.70062\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.70193\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.70247\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.70614\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.70783\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.70902\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.71468\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.71510\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.71601\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.71852\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.71959\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.72083\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.72225\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.72362\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.72432\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.72452\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.72520\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.72596\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.72794\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.73006\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.73011\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.73148\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.73350\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.73540\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.73620\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.73778\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.73797\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.73860\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.73993\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.74065\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.74230\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.74261\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.74299\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.74323\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.74362\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.74401\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.74528\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.74730\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.74833\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.74888\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.74938\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.75210\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.75297\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.75394\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.75450\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.75520\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.75667\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.75704\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.75776\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.75823\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.75884\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.75911\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.75985\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.76039\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.76108\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.76152\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.76184\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.76228\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.76290\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.76324\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.76438\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.76457\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.76533\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.76549\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.76604\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.76677\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.76758\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.76837\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.76976\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.77041\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.77071\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.77108\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.77145\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.77255\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.77288\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.77371\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.77395\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.77438\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.77476\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.77510\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.77536\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.77589\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.77662\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.77754\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.77833\u001b[0m\n",
      "\n",
      "2024-11-01 11:31:52 Uploading - Uploading generated training model\n",
      "2024-11-01 11:31:52 Completed - Training job completed\n",
      "Training seconds: 105\n",
      "Billable seconds: 105\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Automatically retrieve the SageMaker execution role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Use the default S3 bucket for SageMaker\n",
    "s3_bucket_name = sagemaker_session.default_bucket()\n",
    "output_path = f\"s3://{s3_bucket_name}/xgboost/output\"\n",
    "\n",
    "# Step 2: Define and Train XGBoost Model\n",
    "xgb_estimator = Estimator(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", sagemaker_session.boto_region_name, \"1.2-1\"),\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=100,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"auc\"\n",
    ")\n",
    "\n",
    "# Train the model using the training data in S3\n",
    "xgb_estimator.fit({\"train\": TrainingInput(s3_input_train, content_type=\"csv\")})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the XGBoost Model\n",
    "In this step, we deploy the trained XGBoost model to a SageMaker endpoint, allowing for real-time predictions on incoming data using the specified instance type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2024-11-01-11-32-08-606\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2024-11-01-11-32-08-606\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2024-11-01-11-32-08-606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# Step 3: Deploy the Model\n",
    "predictor = xgb_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Cleaning the Test Data\n",
    "This step involves loading the original test data from a CSV file, cleaning it by converting boolean values to integers, ensuring all values are numeric, and dropping any rows with NaN values. The cleaned dataset is then saved to a new CSV file for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to test_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "# Step 1: Load and Clean the Data\n",
    "# Load the original test data file\n",
    "file_path = 'test.csv'  # Replace with the actual file path if needed\n",
    "test_data = pd.read_csv(file_path)\n",
    "\n",
    "# Replace boolean strings 'True'/'False' with 1 and 0 across the entire DataFrame\n",
    "test_data.replace({True: 1, False: 0, 'True': 1, 'False': 0}, inplace=True)\n",
    "\n",
    "# Verify that all values are numeric\n",
    "test_data = test_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop any rows containing NaN values introduced during conversion\n",
    "test_data_cleaned = test_data.dropna()\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_file_path = 'test_cleaned.csv'\n",
    "test_data_cleaned.to_csv(cleaned_file_path, index=False, header=False)\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring the Correct Number of Features in the Test Data\n",
    "In this section, we verify that the test dataset contains the expected number of features required by the model. If the test data has more columns than expected, we retain only the relevant features. If it has fewer columns, a warning is printed. Additionally, any remaining boolean strings are converted to integers, and the adjusted test data is saved to a new CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted test data saved to test_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure we have the correct number of features by checking against the training feature count\n",
    "expected_feature_count = 94  # The feature count expected by the model\n",
    "\n",
    "# Adjust columns if necessary\n",
    "if test_data.shape[1] > expected_feature_count:\n",
    "    test_data = test_data.iloc[:, :expected_feature_count]  # Keep only the first 112 columns\n",
    "elif test_data.shape[1] < expected_feature_count:\n",
    "    print(f\"Warning: Test data has fewer columns ({test_data.shape[1]}) than expected ({expected_feature_count}).\")\n",
    "\n",
    "# Convert 'True'/'False' strings to 1/0 if they still exist\n",
    "test_data.replace({'True': 1, 'False': 0}, inplace=True)\n",
    "\n",
    "# Save the adjusted test data\n",
    "cleaned_file_path = 'test_cleaned.csv'\n",
    "test_data.to_csv(cleaned_file_path, index=False, header=False)\n",
    "print(f\"Adjusted test data saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Feature Names for Training and Test Datasets\n",
    "In this section, we print the names of the features used in the training dataset as well as those present in the test dataset. This helps in verifying the consistency of features across both datasets, ensuring that the model can be applied correctly to the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: Index(['Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3',\n",
      "       'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9',\n",
      "       'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3',\n",
      "       'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7',\n",
      "       'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11',\n",
      "       'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15',\n",
      "       'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19',\n",
      "       'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23',\n",
      "       'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27',\n",
      "       'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31',\n",
      "       'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5',\n",
      "       'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL',\n",
      "       'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN',\n",
      "       'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX',\n",
      "       'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN',\n",
      "       'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO',\n",
      "       'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5',\n",
      "       'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9',\n",
      "       'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12',\n",
      "       'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15',\n",
      "       'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18',\n",
      "       'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21',\n",
      "       'DepHourofDay_22', 'DepHourofDay_23'],\n",
      "      dtype='object')\n",
      "Test features: Index(['0.0', '1440.0', '0', '0.1', '1', '0.2', '0.3', '0.4', '0.5', '0.6',\n",
      "       '0.7', '0.8', '0.9', '1.1', '0.10', '0.11', '0.12', '0.13', '0.14',\n",
      "       '0.15', '0.16', '0.17', '0.18', '0.19', '0.20', '0.21', '1.2', '0.22',\n",
      "       '0.23', '0.24', '0.25', '0.26', '0.27', '0.28', '0.29', '0.30', '0.31',\n",
      "       '0.32', '0.33', '0.34', '0.35', '0.36', '0.37', '0.38', '0.39', '0.40',\n",
      "       '0.41', '0.42', '0.43', '0.44', '0.45', '1.3', '0.46', '0.47', '0.48',\n",
      "       '0.49', '0.50', '0.51', '0.52', '0.53', '0.54', '1.4', '0.55', '0.56',\n",
      "       '0.57', '0.58', '0.59', '0.60', '0.61', '0.62', '1.5', '0.63', '0.64',\n",
      "       '0.65', '0.66', '0.67', '0.68', '0.69', '0.70', '0.71', '0.72', '1.6',\n",
      "       '0.73', '0.74', '0.75', '0.76', '0.77', '0.78', '0.79', '0.80', '0.81',\n",
      "       '0.82', '0.83', '0.84'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Training features:\", X_train.columns)\n",
    "print(\"Test features:\", test_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing and Uploading Test Data for Batch Transformation\n",
    "In this section, we prepare the cleaned test data for batch transformation. We check for any missing features compared to the training dataset and ensure that categorical variables are properly one-hot encoded. After aligning the test data with the expected feature set, we save the adjusted dataset and upload it to the default S3 bucket used by SageMaker. Finally, we initiate a batch transform job to generate predictions based on the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the test dataset: ['0.0', '1440.0', '0', '0.1', '1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.1', '0.10', '0.11', '0.12', '0.13', '0.14', '0.15', '0.16', '0.17', '0.18', '0.19', '0.20', '0.21', '1.2', '0.22', '0.23', '0.24', '0.25', '0.26', '0.27', '0.28', '0.29', '0.30', '0.31', '0.32', '0.33', '0.34', '0.35', '0.36', '0.37', '0.38', '0.39', '0.40', '0.41', '0.42', '0.43', '0.44', '0.45', '1.3', '0.46', '0.47', '0.48', '0.49', '0.50', '0.51', '0.52', '0.53', '0.54', '1.4', '0.55', '0.56', '0.57', '0.58', '0.59', '0.60', '0.61', '0.62', '1.5', '0.63', '0.64', '0.65', '0.66', '0.67', '0.68', '0.69', '0.70', '0.71', '0.72', '1.6', '0.73', '0.74', '0.75', '0.76', '0.77', '0.78', '0.79', '0.80', '0.81', '0.82', '0.83', '0.84']\n",
      "Missing features in the test data: ['Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO', 'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5', 'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9', 'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12', 'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15', 'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18', 'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21', 'DepHourofDay_22', 'DepHourofDay_23']\n",
      "Filtered Test Features: Index(['Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3',\n",
      "       'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9',\n",
      "       'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3',\n",
      "       'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7',\n",
      "       'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11',\n",
      "       'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15',\n",
      "       'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19',\n",
      "       'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23',\n",
      "       'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27',\n",
      "       'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31',\n",
      "       'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5',\n",
      "       'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL',\n",
      "       'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN',\n",
      "       'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX',\n",
      "       'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN',\n",
      "       'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO',\n",
      "       'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5',\n",
      "       'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9',\n",
      "       'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12',\n",
      "       'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15',\n",
      "       'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18',\n",
      "       'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21',\n",
      "       'DepHourofDay_22', 'DepHourofDay_23'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2024-11-01-11-56-13-631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................\u001b[34m[2024-11-01:12:01:33:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:33:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:33:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:33:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:33:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:33:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [20] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [20] [INFO] Listening at: unix:/tmp/gunicorn.sock (20)\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [20] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [20] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [20] [INFO] Listening at: unix:/tmp/gunicorn.sock (20)\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [20] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [30] [INFO] Booting worker with pid: 30\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [30] [INFO] Booting worker with pid: 30\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:12:01:38 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:12:01:38 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:39:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Nov/2024:12:01:38 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Nov/2024:12:01:38 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:39:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2024-11-01T12:01:38.761:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:40:INFO] Loading model file '/opt/ml/model/xgboost-model'\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:12:01:40 +0000] \"POST /invocations HTTP/1.1\" 200 339080 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:40:INFO] Loading model file '/opt/ml/model/xgboost-model'\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Nov/2024:12:01:40 +0000] \"POST /invocations HTTP/1.1\" 200 339080 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n",
      "\u001b[34m[2024-11-01:12:01:33:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:33:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:33:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:33:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:33:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:33:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [20] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [20] [INFO] Listening at: unix:/tmp/gunicorn.sock (20)\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [20] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [20] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [20] [INFO] Listening at: unix:/tmp/gunicorn.sock (20)\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [20] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[34m[2024-11-01 12:01:33 +0000] [30] [INFO] Booting worker with pid: 30\u001b[0m\n",
      "\u001b[35m[2024-11-01 12:01:33 +0000] [30] [INFO] Booting worker with pid: 30\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:12:01:38 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:12:01:38 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:39:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Nov/2024:12:01:38 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Nov/2024:12:01:38 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:39:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2024-11-01T12:01:38.761:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2024-11-01:12:01:40:INFO] Loading model file '/opt/ml/model/xgboost-model'\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Nov/2024:12:01:40 +0000] \"POST /invocations HTTP/1.1\" 200 339080 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2024-11-01:12:01:40:INFO] Loading model file '/opt/ml/model/xgboost-model'\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Nov/2024:12:01:40 +0000] \"POST /invocations HTTP/1.1\" 200 339080 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "Batch transform output saved to: s3://sagemaker-us-east-1-471112926817/xgboost/output\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "\n",
    "# Load the cleaned test data\n",
    "test_data = pd.read_csv(\"test_cleaned.csv\")\n",
    "\n",
    "# Print the original columns for debugging\n",
    "print(\"Columns in the test dataset:\", test_data.columns.tolist())\n",
    "\n",
    "# Define the training feature names again for reference\n",
    "training_feature_names = [\n",
    "    'Distance', 'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3',\n",
    "    'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9',\n",
    "    'Month_10', 'Month_11', 'Month_12', 'DayofMonth_2', 'DayofMonth_3',\n",
    "    'DayofMonth_4', 'DayofMonth_5', 'DayofMonth_6', 'DayofMonth_7',\n",
    "    'DayofMonth_8', 'DayofMonth_9', 'DayofMonth_10', 'DayofMonth_11',\n",
    "    'DayofMonth_12', 'DayofMonth_13', 'DayofMonth_14', 'DayofMonth_15',\n",
    "    'DayofMonth_16', 'DayofMonth_17', 'DayofMonth_18', 'DayofMonth_19',\n",
    "    'DayofMonth_20', 'DayofMonth_21', 'DayofMonth_22', 'DayofMonth_23',\n",
    "    'DayofMonth_24', 'DayofMonth_25', 'DayofMonth_26', 'DayofMonth_27',\n",
    "    'DayofMonth_28', 'DayofMonth_29', 'DayofMonth_30', 'DayofMonth_31',\n",
    "    'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5',\n",
    "    'DayOfWeek_6', 'DayOfWeek_7', 'Reporting_Airline_DL',\n",
    "    'Reporting_Airline_OO', 'Reporting_Airline_UA', 'Reporting_Airline_WN',\n",
    "    'Origin_CLT', 'Origin_DEN', 'Origin_DFW', 'Origin_IAH', 'Origin_LAX',\n",
    "    'Origin_ORD', 'Origin_PHX', 'Origin_SFO', 'Dest_CLT', 'Dest_DEN',\n",
    "    'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD', 'Dest_PHX', 'Dest_SFO',\n",
    "    'DepHourofDay_1', 'DepHourofDay_2', 'DepHourofDay_4', 'DepHourofDay_5',\n",
    "    'DepHourofDay_6', 'DepHourofDay_7', 'DepHourofDay_8', 'DepHourofDay_9',\n",
    "    'DepHourofDay_10', 'DepHourofDay_11', 'DepHourofDay_12',\n",
    "    'DepHourofDay_13', 'DepHourofDay_14', 'DepHourofDay_15',\n",
    "    'DepHourofDay_16', 'DepHourofDay_17', 'DepHourofDay_18',\n",
    "    'DepHourofDay_19', 'DepHourofDay_20', 'DepHourofDay_21',\n",
    "    'DepHourofDay_22', 'DepHourofDay_23',\n",
    "]\n",
    "\n",
    "# Check for missing features\n",
    "missing_features = [feature for feature in training_feature_names if feature not in test_data.columns]\n",
    "if missing_features:\n",
    "    print(f\"Missing features in the test data: {missing_features}\")\n",
    "\n",
    "# Identify categorical columns that are supposed to be one-hot encoded\n",
    "# Adjust this list based on your actual categorical features used in training\n",
    "categorical_columns = [\n",
    "    'Quarter_2', 'Quarter_3', 'Quarter_4', 'Month_2', 'Month_3', 'Month_4',\n",
    "    'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10',\n",
    "    'Month_11', 'Month_12', 'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4',\n",
    "    'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7',\n",
    "    'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA',\n",
    "    'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW',\n",
    "    'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO',\n",
    "    'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX',\n",
    "    'Dest_ORD', 'Dest_PHX', 'Dest_SFO'\n",
    "]\n",
    "\n",
    "# Remove categorical columns from the test set if they are not present\n",
    "categorical_columns = [col for col in categorical_columns if col in test_data.columns]\n",
    "\n",
    "# One-hot encode the categorical columns in the test dataset\n",
    "test_data = pd.get_dummies(test_data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Align the test data with the training feature names\n",
    "test_data_filtered = test_data.reindex(columns=training_feature_names, fill_value=0)\n",
    "\n",
    "# Verify that the test data now has the correct columns\n",
    "print(\"Filtered Test Features:\", test_data_filtered.columns)\n",
    "\n",
    "# Save the filtered test data for transformation\n",
    "updated_test_file_path = 'test_cleaned_updated.csv'\n",
    "test_data_filtered.to_csv(updated_test_file_path, index=False, header=False)\n",
    "\n",
    "# Upload cleaned and filtered test data to S3\n",
    "s3_input_test = sagemaker_session.upload_data(updated_test_file_path, bucket=s3_bucket_name, key_prefix=\"xgboost/test\")\n",
    "\n",
    "# Proceed with batch transform using the updated test data\n",
    "transformer = Transformer(\n",
    "    model_name=predictor.endpoint_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=f\"s3://{s3_bucket_name}/xgboost/output\"\n",
    ")\n",
    "\n",
    "# Perform batch transform\n",
    "transformer.transform(data=s3_input_test, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()\n",
    "\n",
    "print(f\"Batch transform output saved to: s3://{s3_bucket_name}/xgboost/output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up Previous Predictions\n",
    "This code snippet checks for the existence of a file or directory named `predictions.csv` in the current working directory. If it finds a directory with that name, it lists its contents and then removes the entire directory along with all its files. If `predictions.csv` is a file instead, it deletes the file directly. This cleanup step ensures that any old predictions do not interfere with the new predictions being downloaded from the S3 bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Check if 'predictions.csv' exists and is a directory\n",
    "if os.path.exists(\"predictions.csv\"):\n",
    "    if os.path.isdir(\"predictions.csv\"):\n",
    "        # List contents of the directory for inspection\n",
    "        print(\"Contents of 'predictions.csv' directory:\", os.listdir(\"predictions.csv\"))\n",
    "        \n",
    "        # Remove the directory and all its contents\n",
    "        shutil.rmtree(\"predictions.csv\")\n",
    "        print(\"'predictions.csv' directory and its contents have been removed.\")\n",
    "    else:\n",
    "        # If it's a file, remove it directly\n",
    "        os.remove(\"predictions.csv\")\n",
    "        print(\"'predictions.csv' file removed.\")\n",
    "\n",
    "# Proceed with the previous code to download the predictions file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Predictions\n",
    "This code snippet is responsible for evaluating the model predictions after they have been processed by the SageMaker batch transform job. It begins by initializing the Boto3 S3 client and defining the S3 bucket and output prefix to retrieve the generated prediction files. It checks for any existing `predictions.csv` file or directory and cleans up to avoid conflicts.\n",
    "\n",
    "The predictions are then downloaded from S3, and the code verifies that the file was created successfully. It loads the predictions and compares their shape against the ground truth values (`y_test`). In case of inconsistencies in lengths, it adjusts the predictions accordingly. \n",
    "\n",
    "Additionally, it ensures that `y_test` does not contain any NaN values, which could lead to errors during evaluation. The code then calculates various performance metrics, such as accuracy, precision, recall, F1 score, and AUC, and prints them to the console for review. This provides a comprehensive overview of the model's performance on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in output path: ['xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/claim.smd', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/collections/000000000/worker_0_collections.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/events/000000000000/000000000000_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/events/000000000010/000000000010_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/events/000000000020/000000000020_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/events/000000000030/000000000030_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/events/000000000040/000000000040_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/events/000000000050/000000000050_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/events/000000000060/000000000060_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/events/000000000070/000000000070_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/events/000000000080/000000000080_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/events/000000000090/000000000090_worker_0.tfevents', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/index/000000000/000000000000_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/index/000000000/000000000010_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/index/000000000/000000000020_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/index/000000000/000000000030_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/index/000000000/000000000040_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/index/000000000/000000000050_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/index/000000000/000000000060_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/index/000000000/000000000070_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/index/000000000/000000000080_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/index/000000000/000000000090_worker_0.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/debug-output/training_job_end.ts', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/output/model.tar.gz', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/profiler-output/framework/training_job_end.ts', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/profiler-output/system/incremental/2024110111/1730460600.algo-1.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/profiler-output/system/incremental/2024110111/1730460660.algo-1.json', 'xgboost/output/sagemaker-xgboost-2024-11-01-11-29-21-563/profiler-output/system/training_job_end.ts', 'xgboost/output/test_cleaned_updated.csv.out']\n",
      "y_pred shape: (16954,), y_test shape: (16956,)\n",
      "Inconsistent lengths detected: y_pred (16954) vs y_test (16956)\n",
      "y_test contains NaN values. Cleaning up...\n",
      "Predictions distribution (first 10 values): [0.21486647 0.21486647 0.21486647 0.21486647 0.21486647 0.21486647\n",
      " 0.21486647 0.21486647 0.21486647 0.21486647]\n",
      "Binary Predictions distribution: [16625]\n",
      "Performance metrics for Dataset V2:\n",
      "Accuracy: 0.75\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n",
      "AUC: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Initialize Boto3 S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Define S3 bucket and output prefix\n",
    "s3_bucket_name = sagemaker_session.default_bucket()\n",
    "output_prefix = \"xgboost/output\"\n",
    "\n",
    "# List files in the output directory to find the batch transform output file (.out)\n",
    "response = s3_client.list_objects_v2(Bucket=s3_bucket_name, Prefix=output_prefix)\n",
    "output_files = [content['Key'] for content in response.get('Contents', [])]\n",
    "print(\"Files in output path:\", output_files)\n",
    "\n",
    "# Locate the file ending in '.out' (the predictions file)\n",
    "predictions_file_key = next((file for file in output_files if file.endswith('.out')), None)\n",
    "\n",
    "# Clean up any pre-existing 'predictions.csv' directory or file to avoid conflicts\n",
    "if os.path.exists(\"predictions.csv\"):\n",
    "    if os.path.isdir(\"predictions.csv\"):\n",
    "        shutil.rmtree(\"predictions.csv\")  # Recursively remove the directory\n",
    "    else:\n",
    "        os.remove(\"predictions.csv\")  # Remove the file if it exists\n",
    "\n",
    "if predictions_file_key:\n",
    "    # Use a unique temporary name for download\n",
    "    temp_file = \"temp_predictions.csv\"\n",
    "    s3_client.download_file(s3_bucket_name, predictions_file_key, temp_file)\n",
    "    \n",
    "    # Rename the temporary file to 'predictions.csv'\n",
    "    os.rename(temp_file, \"predictions.csv\")\n",
    "\n",
    "    # Verify that 'predictions.csv' was created successfully\n",
    "    if os.path.isfile(\"predictions.csv\"):\n",
    "        try:\n",
    "            # Load predictions, skip empty lines\n",
    "            y_pred = np.loadtxt(\"predictions.csv\", delimiter=\",\")\n",
    "            y_pred = y_pred.flatten()\n",
    "\n",
    "            # Check the shape of y_pred and y_test\n",
    "            print(f\"y_pred shape: {y_pred.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "            # Ensure lengths match between y_pred and y_test\n",
    "            if len(y_pred) != len(y_test):\n",
    "                print(f\"Inconsistent lengths detected: y_pred ({len(y_pred)}) vs y_test ({len(y_test)})\")\n",
    "                if len(y_pred) > len(y_test):\n",
    "                    y_pred = y_pred[:len(y_test)]\n",
    "                else:\n",
    "                    y_pred = np.pad(y_pred, (0, len(y_test) - len(y_pred)), 'constant', constant_values=0)\n",
    "\n",
    "            # Ensure y_test does not contain NaN values\n",
    "            if np.any(np.isnan(y_test)):\n",
    "                print(\"y_test contains NaN values. Cleaning up...\")\n",
    "                mask = ~np.isnan(y_test)\n",
    "                y_test = y_test[mask]\n",
    "                y_pred = y_pred[mask]\n",
    "\n",
    "            # Print out the prediction values to understand the distribution\n",
    "            print(\"Predictions distribution (first 10 values):\", y_pred[:10])\n",
    "            \n",
    "            # Dynamically find a threshold if needed, but start with a default of 0.5\n",
    "            threshold = 0.5\n",
    "            y_pred_binary = np.where(y_pred >= threshold, 1, 0)\n",
    "\n",
    "            # Check distribution of binary predictions\n",
    "            print(\"Binary Predictions distribution:\", np.bincount(y_pred_binary))\n",
    "\n",
    "            # Define a function to calculate and print evaluation metrics\n",
    "            def print_metrics(y_true, y_pred_binary):\n",
    "                print(f\"Accuracy: {accuracy_score(y_true, y_pred_binary):.2f}\")\n",
    "                print(f\"Precision: {precision_score(y_true, y_pred_binary):.2f}\")\n",
    "                print(f\"Recall: {recall_score(y_true, y_pred_binary):.2f}\")\n",
    "                print(f\"F1 Score: {f1_score(y_true, y_pred_binary):.2f}\")\n",
    "                print(f\"AUC: {roc_auc_score(y_true, y_pred):.2f}\")  # Use continuous y_pred for AUC\n",
    "\n",
    "            # Display performance metrics for the model on the test dataset\n",
    "            print(\"Performance metrics for Dataset V2:\")\n",
    "            print_metrics(y_test, y_pred_binary)\n",
    "        \n",
    "        except ValueError as e:\n",
    "            print(f\"Error loading predictions from 'predictions.csv': {e}\")\n",
    "    else:\n",
    "        print(\"Download failed or predictions.csv does not exist.\")\n",
    "else:\n",
    "    print(\"Prediction file (.out) not found. Please check the batch transform job for errors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metrics for Dataset V2 indicate a concerning situation regarding the predictive model's effectiveness. With an accuracy of 0.75, the model appears to correctly classify a significant proportion of instances; however, this figure alone does not tell the full story. The precision is reported as 0.00, indicating that when the model predicts a positive outcome, it fails to do so correctly. This suggests that all positive predictions are false positives, which can lead to a complete breakdown in trust for any positive classifications made by the model.\n",
    "\n",
    "Moreover, the recall is also 0.00, implying that the model is unable to identify any actual positive cases present in the dataset. This is a critical flaw, especially in contexts where identifying positives is essential, such as in medical diagnoses or fraud detection. Consequently, the F1 score, which combines precision and recall into a single metric, is also 0.00, further highlighting the model's inadequacy.\n",
    "\n",
    "The Area Under the Receiver Operating Characteristic Curve (AUC) is 0.50, suggesting that the model's ability to distinguish between classes is no better than random chance. Collectively, these metrics point to a need for significant improvement in the model's training process or feature engineering to enhance its predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: sagemaker-xgboost-2024-11-01-09-36-50-451\n",
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-xgboost-2024-11-01-09-36-50-451\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Clean up the endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comment \n",
    "In comparing the Linear Learner and XGBoost models for predicting flight delays, we see distinct advantages and trade-offs that influence their suitability based on factors like computational resources, accuracy, interpretability, and scalability.\n",
    "\n",
    "### Resource Allocation and Computational Complexity\n",
    "The Linear Learner model is a lightweight option, requiring minimal computational resources and thus allowing for faster deployment and iteration. This simplicity makes it ideal for environments with limited computational power or time constraints. However, this models simplicity may limit its ability to accurately capture the complexity of flight delay data, which includes intricate factors like seasonal weather patterns and high variability across airports.\n",
    "\n",
    "XGBoost, on the other hand, is a more advanced ensemble method known for its accuracy, particularly with complex datasets. This model is computationally demanding due to its iterative, error-correcting process, which requires more memory and time. For projects that demand high accuracy, XGBoosts ability to model complex patterns is invaluable, but it may not be ideal in time-sensitive or resource-constrained scenarios.\n",
    "\n",
    "### Accuracy and Model Performance\n",
    "The Linear Learners simplicity generally limits its capacity to model non-linear relationships and interactions within the data, leading to the risk of underfitting. This could result in lower prediction accuracy, as the model might miss important nuances in the data. XGBoost, however, excels in capturing these intricate patterns through its boosting technique, which corrects errors iteratively and allows it to handle complex feature interactions more effectively. This often leads to higher predictive accuracy but requires careful tuning to avoid overfitting, where the model may perform well on training data but struggle with new, unseen data.\n",
    "\n",
    "### Interpretability and Practical Insights\n",
    "Interpretability is crucial for understanding model predictions, especially in high-stakes areas like air travel. Linear Learner models are more interpretable since they provide clear, direct relationships between features and predictions. This transparency allows stakeholders to understand the primary drivers of delays and make informed decisions based on insights into factors such as weather or seasonal changes.\n",
    "\n",
    "XGBoost, although typically more accurate, is less interpretable due to its ensemble structure, which uses multiple decision trees. Understanding predictions from XGBoost often requires additional interpretability tools, like SHAP values, which can make analysis more complex. While this added complexity may be justified in exchange for the models superior performance, it can pose challenges when straightforward explanations are needed for stakeholders.\n",
    "\n",
    "### Scalability and Deployment Considerations\n",
    "In deployment, Linear Learners simplicity supports scalability, especially in cases where rapid retraining or frequent deployment is required. This models minimal resource requirements also make it resilient in real-time applications. However, its inability to adapt to complex, evolving patterns over time can limit its long-term utility.\n",
    "\n",
    "Conversely, XGBoosts robustness to complex data makes it more adaptable to changes over time, making it better suited for long-term production in dynamic environments. While the models deployment may require a more powerful infrastructure, its more likely to maintain accuracy across shifts in data patterns, making it a strong candidate for applications prioritizing long-term predictive reliability.\n",
    "\n",
    "### Conclusion\n",
    "In summary, the Linear Learner offers simplicity, speed, and transparency, making it suitable for baseline modeling or when resources are constrained. However, its limited accuracy may hinder its effectiveness in complex applications. XGBoost provides enhanced accuracy and flexibility, valuable in environments where high performance is critical, though it requires more resources and interpretability tools. The final choice between these models depends on balancing resource availability, accuracy needs, interpretability, and the specific demands of the application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
